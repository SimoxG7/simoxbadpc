{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Classificatori-naive-Bayes\" data-toc-modified-id=\"Classificatori-naive-Bayes-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Classificatori naive Bayes</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "header": true
   },
   "source": [
    "<div class=\"header\">\n",
    "D. Malchiodi, Superhero data science. Vol 1: probabilità e statistica: Naive Bayes.\n",
    "</div>\n",
    "<hr style=\"width: 90%;\" align=\"left\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div id=\"h-0\"></div>\n",
    "\n",
    "# Classificatori naive Bayes\n",
    "\n",
    "Una semplice applicazione del teorema di Bayes permette di costruire un particolare classificatore che prende il nome di _classificatore naive Bayes_. Consideriamo il caso più semplice possibile in cui abbiamo osservato in una serie di individui la presenza o l'assenza di una determinata proprietà, ottenendo per ognuno di essi un'osservazione $x_i \\in \\{ 0, 1 \\}$. Ipotizziamo inoltre di avere associato a ogni individuo un'etichetta $y_i \\in \\{ 0, 1 \\}$ che denota l'appartenenza o meno a una data classe. Prendendo ad esempio il dataset dei supereroi, supponiamo che la proprietà osservata faccia riferimento all'avere oppure no gli occhi neri (indicando con $x_i = 1$ il fatto di avere gli occhi neri e con $x_i = 0$ il fatto di averli di un altro colore), e la classe considerata indichi se l'editore corrispondente sia o meno la Marvel Comics (anche in questo caso, denotando con $y_i = 1$ il fatto di essere un supereroe Marvel e con $y_i = 0$ il fatto di non esserlo).\n",
    "\n",
    "Indichiamo inoltre con $N$ l'evento «un supereroe preso a caso ha gli occhi neri» e con $M$ l'evento «un supereroe preso a caso è un supereroe Marvel»: grazie al teorema di Bayes sappiamo che vale\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathrm P(M|N) = \\frac{\\mathrm P(N|M) \\mathrm P(M)}{\\mathrm P(N)}.\n",
    "\\end{equation}\n",
    "\n",
    "Ora, se a partire dal dataset fosse possibile ottenere una stima delle probabilità che compaiono al secondo membro di questa equazione, sarebbe quindi immediato ottenere una stima di $\\mathrm P(M|N)$ da utilizzare per dire qualcosa sul fatto che un generico supereroe con gli occhi neri sia oppure no un supereroe Marvel. In particolare, se il valore ottenuto fosse sufficientemente alto potremmo spingerci oltre e dire che d'ora in avanti potremo _classificare_ tutti i supereroi con gli occhi neri di cui non conosciamo l'editore come supereroi Marvel. Allo stesso modo, se il valore ottenuto fosse particolarmente basso potremmo comportarci in modo analogo, concludendo però che gli occhi neri indicano un editore diverso dalla Marvel.\n",
    "\n",
    "Il fatto di poter azzardare l'editore potrebbe sembrare poco utile, visto che in fondo si tratta di una colonna del nostro dataset e quindi noi già sappiamo quali siano i valori per questo attributo. In realtà sappiamo che ci potrebbero essere dei valori mancanti, oppure potremmo in futuro avere a disposizione nuovi dati per i quali questa informazione non è disponibile.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "Va notato che la scelta di usare il colore degli occhi come proprietà osservata e la casa editrice come classe è assolutamente arbitraria. In modo analogo avremmo potuto ragionare in termini di come il fatto di essere un supereroe Marvel possa essere utilizzato per azzardare il colore degli occhi, scambiando gli eventi $M$ e $N$ nelle formule.\n",
    "</div>\n",
    "\n",
    "Un modo semplice per stimare le probabilità indicate è quello di osservare con che frequenza gli eventi associati si verificano nel dataset a nostra disposizione. In particolare,\n",
    "\n",
    "- la probabilità $\\mathrm P(N|M)$ può essere approssimata calcolando la frequenza relativa con cui un supereroe Marvel del dataset ha gli occhi neri;\n",
    "- la probabilità $\\mathrm P(M)$ può essere approssimata calcolando la frequenza relativa con cui un supereroe del dataset è edito dalla Marvel;\n",
    "- la probabilità $\\mathrm P(N)$ può essere approssimata calcolando la frequenza relativa con cui un supereroe del dataset ha gli occhi neri.\n",
    "\n",
    "Attrezziamoci quindi per poter calcolare queste frequenze, caricando il dataset e importando le librerie che utilizzeremo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "heroes = pd.read_csv('data/heroes.csv', sep=';', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Possiamo ora approssimare la probabilità dell'evento $M$ come la frequenza relativa del medesimo evento sul dataset, e quindi calcolando il rapporto tra il numero di supereroi Marvel e il totale dei supereroi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.527891156462585"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "marvel_heroes = heroes[heroes['Publisher']=='Marvel Comics']\n",
    "p_marvel = len(marvel_heroes) / len(heroes)\n",
    "p_marvel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Allo stesso modo, la probabilità $\\mathrm P(N)$ si approssima facilmente calcolando la frazione di supereroi del dataset che hanno gli occhi neri."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0326530612244898"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "black_eyes_heroes = heroes[heroes['Eye color']=='Black']\n",
    "p_blackeyes = len(black_eyes_heroes) / len(heroes)\n",
    "p_blackeyes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Infine, una stima della probabilità $\\mathrm P(N|M)$ si ottiene dividendo il numero di supereroi Marvel che hanno gli occhi neri per il numero totale di supereroi Marvel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.028350515463917526"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blackeyes_and_marvel_heroes = heroes[(heroes['Publisher']=='Marvel Comics') & \\\n",
    "                                     (heroes['Eye color']=='Black')]\n",
    "\n",
    "p_blackeyes_given_marvel = len(blackeyes_and_marvel_heroes) / len(marvel_heroes)\n",
    "p_blackeyes_given_marvel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applicando il teorema di Bayes si ottiene quindi che la probabilità di trovarsi di fronte a un supereroe Marvel, dato che questo ha gli occhi neri, può essere approssimata come segue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.45833333333333326"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_blackeyes_given_marvel * p_marvel / p_blackeyes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dunque sembrerebbe che gli occhi neri siano leggermente più probabili nei supereroi **non** Marvel. Ovviamente avremmo ottenuto un risultato coerente se avessimo applicato lo stesso procedimento per stimare la probabilità $\\mathrm(\\overline M|N)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5416666666666665"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nonmarvel_heroes = heroes[heroes['Publisher']!='Marvel Comics']\n",
    "blackeyes_and_nonmarvel_heroes = heroes[(heroes['Publisher']!='Marvel Comics') & \\\n",
    "                                        (heroes['Eye color']=='Black')]\n",
    "\n",
    "p_notmarvel = len(nonmarvel_heroes) / len(heroes)\n",
    "p_blackeyes_given_notmarvel = len(blackeyes_and_nonmarvel_heroes) / len(nonmarvel_heroes)\n",
    "\n",
    "p_blackeyes_given_notmarvel * p_notmarvel / p_blackeyes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visto che le due probabilità ottenute sono molto vicine a $\\frac{1}{2}$, ha senso valutare se l'uso di diverse proprietà da osservare _congiuntamente_ ci possa portare a una procedura più precisa. Per esempio potremmo tener conto sia del colore degli occhi, sia di quello dei capelli, senza fissare un colore specifico. Consideriamo pertanto tutti i possibili valori $\\{ o_1, o_2, \\dots, o_n \\}$ per i colori degli occhi e scriviamo $O = o_i$ per indicare l'evento «un supereroe a caso ha gli occhi del colore $o_i$». Procediamo in modo analogo per l'evento $C = c_j$ che equivale a «un supereroe a caso ha i capelli del colore $c_j$», dove i valori possibili ora corrispondono all'insieme $\\{ c_1, c_2, \\dots, c_m \\}$. Il teorema di Bayes permette in questo caso di scrivere\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathrm P(M|O = o_i \\cap C = c_j) = \\frac{\\mathrm P(O = o_i \\cap C = c_j | M) \\mathrm P(M)}{\\mathrm P((O = o_i \\cap C = c_j)}.\n",
    "\\end{equation}\n",
    "\n",
    "Pertanto si può pensare di estendere il ragionamento visto precedentemente: dato un supereroe di cui si conoscono i colori $o_i$ e $c_j$ di occhi e capelli, si approssimano le probabilità al secondo membro dell'equazione e in funzione del valore ottenuto si classifica il supereroe come «Marvel» oppure come «non Marvel». I classificatori _naive Bayes_ semplificano il lavoro approssimando $\\mathrm P(O = o_i \\cap C = c_j | M)$ con la quantità $\\mathrm P(O = o_i | M) \\mathrm P(C = c_j | M)$. Procedendo in questo modo, nel numeratore della frazione si calcoleranno al più $m + n$ diverse probabilità al posto di $m n$ (più avanti vedremo come evitare l'analogo problema quando si calcola il denominatore). L'aggettivo _naive_ nel nome del classificatore sottolinea la semplicità dell'ipotesi adottata (_naive_ significa infatti «ingenuo» in inglese).\n",
    "\n",
    "Concentriamoci per esempio su un supereroe con occhi e capelli neri: la probabilità che esso sia un supereroe Marvel sarà quindi calcolata da un classificatore naive Bayes come\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathrm P(M|O = \\text{neri} \\cap C = \\text{neri}) \\approx \\frac{\\mathrm P(O = \\text{neri} | M) \\mathrm P(C = \\text{neri} | M) \\mathrm P(M)}{\\mathrm P((O = \\text{neri} \\cap C = \\text{neri})}.\n",
    "\\end{equation}\n",
    "\n",
    "Alcune delle probabilità coinvolte sono già state calcolate. Nella cella seguente quelle mancanti sono derivate in modo analogo, ottenendo come risultato una bassa probabilità di avere a che fare con un supereroe Marvel se i suoi occhi e i suoi capelli sono neri."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2190721649484536"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "black_eyes_and_hair_heroes = heroes[(heroes['Eye color']=='Black') & \\\n",
    "                                    (heroes['Hair color']=='Black')]\n",
    "\n",
    "p_blackeyes_and_black_hair = len(black_eyes_and_hair_heroes) / len(heroes)\n",
    "\n",
    "blackhair_and_marvel_heroes = heroes[(heroes['Publisher']=='Marvel Comics') & \\\n",
    "                                     (heroes['Hair color']=='Black')]\n",
    "p_blackhair_given_marvel = len(blackhair_and_marvel_heroes) / len(marvel_heroes)\n",
    "        \n",
    "p_blackeyes_given_marvel * p_blackhair_given_marvel * p_marvel \\\n",
    "     / p_blackeyes_and_black_hair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consideriamo infine il caso più generale in cui vi sono più di due classi su cui suddividere gli individui. Nell'esempio che stiamo considerando stiamo quindi dicendo che vi sono valori diversi $\\{ e_1, \\dots, e_o \\}$ per l'editore, e quindi saremo interessati a eventi della forma $E = e_k$. Idealmente, una volta osservato un supereroe di cui $o_j$ e $c_j$ indicano rispettivamente il colore di occhi e capelli sarà necessario calcolare $\\mathrm P(E = e_k | O = o_i \\cap C = c_j)$ per tutti i possibili $e_k$: il più alto ottenuto individuerà l'editore da associare al supereroe. È però possibile semplificare ulteriormente il procedimento, notando che esso consiste nel determinare, al variare di $k$, il più alto tra i valori\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathrm P(E = e_k|O = o_i \\cap C = c_j) \\approx \\frac{\\mathrm P(O = o_i | E = e_k) \\mathrm P(C = c_j | E = e_k) \\mathrm P(E = e_k)}{\\mathrm P((O = o_i \\cap C = c_j)}.\n",
    "\\end{equation}\n",
    "\n",
    "Ora, il denominatore è indipendente da $k$ e quindi la classificazione si può effettuare trovando il valore $k$ che massimizza la quantità\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathrm P(O = o_i | E = e_k) \\mathrm P(C = c_j | E = e_k) \\mathrm P(E = e_k).\n",
    "\\end{equation}\n",
    "\n",
    "Riformuliamo il ragionamento effettuato in modo da renderlo il più generale possibile: immaginiamo che per un dato individuo si osservino congiuntamente i valori $x_1, \\dots, x_n$ per $n$ proprietà $X_1, \\dots, X_n$, e denotiamo con $X_1 = x_1, \\dots, X_n=x_n$ gli eventi corrispondenti. Supponiamo inoltre che vi siano $m$ possibili classi che corrispondono agli eventi $Y = y_1, \\dots, Y = y_m$. Per ognuna di queste classi vorremo calcolare\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathrm P(Y=y_k | X_1=x_1, \\dots, X_n=x_n) = \\frac{\\mathrm P(X_1=x_1, \\dots, X_n=x_n | Y=y_k) \\mathrm P(Y=y_k)}{\\mathrm P(X_1=x_1, \\dots, X_n=x_n)}\n",
    "\\end{equation}\n",
    "\n",
    "e assegnare l'individuo alla classe per cui tale probabilità è massima. L'approssimazione alla base del classificatore naive Bayes ci permette di scrivere\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathrm P(Y=y_k | X_1=x_1, \\dots, X_n=x_n) \\approx \\frac{\\mathrm P(Y=y_k)\\displaystyle\\prod_{i=1}^n\\mathrm P(X_i=x_i | Y=y_k)}{\\mathrm P(X_1=x_1, \\dots, X_n=x_n)},\n",
    "\\end{equation}\n",
    "\n",
    "e tenuto conto che anche in questo caso il denominatore non dipende da $k$, potremo associare l'individuo alla classe $y_{k^*}$, dove\n",
    "\n",
    "\\begin{equation}\n",
    "k^* = \\arg\\max_k \\mathrm P(Y = y_k) \\prod_{i=1}^n \\mathrm P(X_i = x_i | Y = y_k).\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In realtà non è necessario effettuare «a mano» i calcoli legati a un classificatore naive Bayes. Come già visto per gli alberi di decisione, la libreria sklearn mette a disposizione delle classi che permettono di ottenere direttamente un classificatore a partire dai dati iniziali. Per esemplificarne il funzionamento, estraiamo dal nostro dataset alcune colonne ed eliminiamo tutte le righe in cui occorre almeno un valore mancante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['Height', 'Weight', 'Gender', 'First appearance',\n",
    "            'Hair color', 'Eye color', 'Strength', 'Intelligence']\n",
    "heroes_cleaned = heroes.dropna()\n",
    "X = heroes_cleaned[features].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per gli stessi motivi visti quando abbiamo lavorato con gli alberi di decisione, è necessario utilizzare solamente dati numerici. Possiamo convertire gli attributi categorici che corrispondono a genere e colore di occhi e capelli riutilizzando le classi `LabelEncoder`. Procediamo poi a convertire i valori degli attributi rimanenti nel modo che segue: dividiamo innanzitutto l'intervallo di variazione dei corrispondenti valori in quattro intervalli, utilizzando i quattro quartili come estremi; associamo successivamente un diverso valore numerico a ogni intervallo ottenuto e trasformiamo ogni osservazione nel numero che corrisponde all'intervallo individuato. Questa operazione viene effettuata automaticamente dalla funzione `pd.qcut`, a cui passiamo le osservazioni, il numero di intervalli da considerare e i valori numerici da associare a questi ultimi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "n = 4\n",
    "X['Height'] = pd.qcut(X['Height'], n, labels=range(n))\n",
    "X['Weight'] = pd.qcut(X['Weight'], n, labels=range(n))\n",
    "\n",
    "gender_encoder = LabelEncoder()\n",
    "gender_encoder.fit(X['Gender'])\n",
    "X['Gender'] = gender_encoder.transform(X['Gender'])\n",
    "\n",
    "X['First appearance'] = pd.qcut(X['First appearance'],\n",
    "                                n,\n",
    "                                labels=range(n))\n",
    "\n",
    "hair_encoder = LabelEncoder()\n",
    "hair_encoder.fit(X['Hair color'])\n",
    "X['Hair color'] = hair_encoder.transform(X['Hair color'])\n",
    "\n",
    "eye_encoder = LabelEncoder()\n",
    "eye_encoder.fit(X['Eye color'])\n",
    "X['Eye color'] = eye_encoder.transform(X['Eye color'])\n",
    "\n",
    "X['Strength'] = pd.qcut(X['Height'], n, labels=range(n))\n",
    "X['Intelligence'] = pd.qcut(X['Height'], n, labels=range(n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'ultimo ingrediente mancante è l'insieme dei valori per le classi. Per semplificarci la vita, ritorniamo alla classificazione binaria inizialmente considerata: anche in questo caso è necessario lavorare con valori numerici, quindi costruiamo una serie i cui valori sono `1` in corrispondenza dei supereroi Marvel e `0` per tutti gli altri casi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = (heroes_cleaned['Publisher']=='Marvel Comics').apply( \\\n",
    "         lambda x: 1 if x else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In realtà sklearn implementa diverse varianti del classificatore naive Bayes, ognuna delle quali stima in modo diverso le probabilità $\\mathrm P(X_i = x_i | Y = y_k)$. Consideriamo per esempio la classe `GaussianNB`, che ipotizza che tali probabilità si possano calcolare utilizzando una distribuzione gaussiana (uno dei modelli teorici che vedremo tra qualche lezione). La costruzione del classificatore viene fatta usando lo stesso procedimento visto per gli alberi di decisione: si crea un oggetto della classe `GaussianNB`, e poi si invoca su di esso il metodo `fit` specificando come argomenti le osservazioni e le corrispondenti etichette. Dopo sarà sufficiente invocare il metodo `predict` passando una lista di oggetti da classificare. Ricordando che per avere una valutazione imparziale di un classificatore è necessario utilizzare dati non utilizzati per costruire il classificatore stesso, per semplicità limitiamoci a verificare che cosa succeda con le osservazioni in `X`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,\n",
       "       1, 1, 1, 1, 0, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "model = GaussianNB()\n",
    "model.fit(X, Y)\n",
    "\n",
    "model.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il risultato non pare molto confortante, in quanto quasi tutti i casi vengono associati alla Marvel. Per analizzare meglio i risultati, vediamo separatamente come vengono classificati i supereroi delle due classi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X[Y==0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1,\n",
       "       1, 1, 1])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X[Y==1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A partire dai vettori ottenuti è facile calcolare separatamente il numero di veri positivi, falsi positivi, falsi negativi e veri negativi. È anche possibile costruire un dataframe che contenga la corrispondente matrice di confusione."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Classif. positiva</th>\n",
       "      <th>Classif. negativa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Etichetta positiva</th>\n",
       "      <td>110</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Etichetta negativa</th>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Classif. positiva  Classif. negativa\n",
       "Etichetta positiva                110                  3\n",
       "Etichetta negativa                 28                  1"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tp = sum(model.predict(X[Y==1]))\n",
    "fn = len(model.predict(X[Y==1])) - tp\n",
    "fp = sum(model.predict(X[Y==0]))\n",
    "tn = len(model.predict(X[Y==0])) - fp\n",
    "pd.DataFrame([[tp, fn],\n",
    "              [fp, tn]],\n",
    "              index=['Etichetta positiva', 'Etichetta negativa'],\n",
    "              columns=['Classif. positiva', 'Classif. negativa'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La matrice evidenzia una buona classificazione dei supereroi Marvel e una performance molto bassa per quel che riguarda i rimanenti supereroi. Volendo riassumere in un unico indice la bontà del risultato ottenuto è possibile calcolare la percentuale di dati corretti."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7816901408450704"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(tp + tn) / len(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tale percentuale di per sé ci dice che poco meno dell'80% dei casi è stata classificata correttamente. Ciò è dovuto al fatto che nel dataset considerato vi sono in proporzione molti più supereroi Marvel rispetto a quelli delle altre case editrici."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "footer": true
   },
   "source": [
    "<hr style=\"width: 90%;\" align=\"left\" />\n",
    "<span style=\"font-size: 0.8rem;\">D. Malchiodi, Superhero data science. Vol 1: probabilità e statistica: Naive Bayes, 2017.</span>\n",
    "<br>\n",
    "<span style=\"font-size: 0.8rem;\">Powered by <img src=\"img/jupyter-logo.png\" style=\"height: 1rem; display: inline; margin-left: 0.5ex; margin-top: 0;\" alt=\"Jupyter Notebook\"></span>\n",
    "<div style=\"float: left; margin-top: 1ex;\">\n",
    "<img src=\"http://mirrors.creativecommons.org/presskit/icons/cc.large.png\" style=\"width: 1.5em; float: left; margin-right: 0.6ex; margin-top: 0;\">\n",
    "<img src=\"http://mirrors.creativecommons.org/presskit/icons/by.large.png\" style=\"width: 1.5em; float: left; margin-right: 0.6ex; margin-top: 0;\">\n",
    "<img src=\"http://mirrors.creativecommons.org/presskit/icons/nc.large.png\" style=\"width: 1.5em; float: left; margin-right: 0.6ex; margin-top: 0;\">\n",
    "<img src=\"http://mirrors.creativecommons.org/presskit/icons/nd.large.png\" style=\"width: 1.5em; float: left; margin-right: 0.6ex; margin-top: 0;\">\n",
    "<span style=\"font-size: 0.7rem; line-height: 0.7rem; vertical-align: middle;\">Quest'opera è distribuita con Licenza <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-nd/4.0/\">Creative Commons Attribuzione - Non commerciale - Non opere derivate 4.0 Internazionale</a></span>.\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
